# 网易号

> 由我个人负责，带领6人后台团队，加上前端测试，共12人

### 数据增长

几千用户增长到百万用户

每天千篇文章增长到每天百万

### 业务扩张

用户入驻+发文

星级体系、收益体系、质量体系、信用体系、原创体系

文章、视频、图集、直播、讲讲

> 审核状态、编码状态独立开，降低了业务逻辑复杂性，还使得使用上更加直观
>
> 网易号视频业务逻辑的开发，推动了公司视频服务的架构调整
>
> (体现不满足现状，把事情向合理的方向发展)

### 架构调整

服务很老，问题很多，很多机制不健全

原有架构是所有业务功能全在同一个集群部署

变更造成的影响很大，没有健康检查，无法平滑上线。

无法适应业务的快速迭代，以及数据的告诉增长

- 服务拆分，用户后台，管理后台拆分开来，独立部署，互不影响，提升了后台应用的稳定性

- 客户端profile请求依赖，新增大量请求。新建内部服务集群，针对性优化，大量请求不影响用户使用。
- 审核、视频编码等实时性要求较低的业务逻辑，由HTTP方式改为异步队列实现，解耦上下游服务
- 引入ElasticSearch，建立外部索引，解决数据量暴增后，对数据库大量字段筛选排序的需求，优化了接口性能。公司内首个将ES应用到线上业务。
- 优化ES使用，实时写请求通过缓存优化为异步写请求，异步队列+批量处理，减少ES写请求量，降低ES机器压力。增加缓存，减少ES读请求压力。

- 引入canal，简化数据同步流程，减轻开发压力，提高数据一致性。

